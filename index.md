# Portfolio

---
#### [Build ETL Pipeline with PySpark](https://github.com/vidush5/Portfolio-Projects/tree/main/Build%20ETL%20Pipeline%20with%20PySpark)
<img src="images/project01.png?raw=true"/>
This project involves building an ETL pipeline using Pyspark to extract data from a SQL Server database and load it into a PostgreSQL database. Pyspark's data processing features will be utilized to perform efficient data transformations. The aim is to develop a scalable and efficient ETL pipeline for seamless data integration between the two databases.

<br>
[Check out the Code](https://github.com/vidush5/Portfolio-Projects/tree/main/Build%20ETL%20Pipeline%20with%20PySpark)
<br>
<!-- <form action="https://share.streamlit.io/anirbansaha96/asl/main/src/st_asl.py" method="get" target="_blank"><button type="submit">Try it out!</button></form> -->
<br>

---
#### [End to End Automated Data Pipeline using Snowflake, AWS & Airflow](https://github.com/vidush5/Portfolio-Projects/tree/main/End%20to%20End%20Automated%20Data%20Pipeline%20using%20Snowflake%2C%20AWS%20%26%20Airflow)
<img src="images/project02.png?raw=true"/>
The End to End Automated Data Pipeline project aims to streamline the data processing workflow by leveraging the power of Snowflake, AWS, and Airflow. With this pipeline, users can easily extract, transform, and load data from various sources into a Snowflake data warehouse, while automating the entire process using Airflow. This project aims to reduce the time and effort required for data processing and analysis, while ensuring data accuracy and consistency.
<br>
[Check out the Code](https://github.com/vidush5/Portfolio-Projects/tree/main/End%20to%20End%20Automated%20Data%20Pipeline%20using%20Snowflake%2C%20AWS%20%26%20Airflow)
<br>
<!-- <form action="https://share.streamlit.io/anirbansaha96/asl/main/src/st_asl.py" method="get" target="_blank"><button type="submit">Try it out!</button></form> -->
<br>

---


